# V1 Spec: Agent Harness Matrix

## Scope (V1)

Included:

- Open-source **CLI/TUI coding-agent harnesses**: `codex`, `pi-mono`, `opencode`
- “Harness facts” we can justify from **repo code + repo docs**

Excluded for V1:

- Closed-source products (Cursor, Devin, etc.)
- Hosted agents / SaaS behavior that can’t be attributed to code we can read
- Performance claims unless we can cite reproducible benchmarks

## What We’re Building (V1)

1. A **canonical comparison dataset** (one entry per harness).
2. A **field-level evidence system** (every claim has citations + confidence).
3. A **picker-oriented view model**: a small set of high-signal fields that help users choose quickly.

We will build the UI later; the dataset is the foundation.

## Primary User Story

“I want to pick a harness to do real coding work. I care about reliability and control. Show me which harness is likely to work for my environment and preferences.”

## Comparison Categories (V1)

### 1) Edit Mechanism (highest leverage)

Capture how the harness expresses and applies edits:

- Edit tool type (e.g., `apply_patch`, `str_replace`, rewrite, anchored edits)
- Anchoring strategy (exact text match, diff context, hashes/IDs, line numbers)
- Verification behavior (reject on mismatch, best-effort merge, partial apply)
- Common failure modes exposed in docs/issues (e.g., “string not found”)
- Retries / repair loop behavior (does it auto-retry? how many?)

### 2) Tools & Tooling Layer

- Tool list (read/edit/write/exec/search/test, etc.)
- Tool schemas (JSON schema, ad-hoc string protocols, MCP, etc.)
- Error shaping (do tools return structured errors? human-readable?)
- Sandboxing of tools (what is restricted; is it configurable?)

### 3) Config & Extensibility

- Config file location(s) and precedence
- Env vars supported
- Skills/plugins system: where skills live, how loaded, how scoped
- Custom tools: is there a plugin API? MCP? scripts?

### 4) Model Providers & Auth

- Supported providers (OpenAI, Anthropic, etc.) if explicit in code/docs
- Auth methods (env vars, login flows)
- Model selection UX (flags, config, interactive)

### 5) UX & Workflow

- Interaction style (TUI/CLI, streaming, review flows)
- Safety controls (confirmations, approval modes)
- Multi-agent or subagent support (if present)

## Confidence & Evidence Rules

Every field must include:

- `confidence`: `high | medium | low`
- `evidence[]`: at least 1 citation, ideally 2+ (docs + code)

We prefer:

- Code citations for “what it actually does”
- Doc citations for “what it promises”
- Issue citations only for “known problem reports”, labeled clearly

## Output Artifacts (V1)

- `agents/<id>.yaml` entries (generated by collector agent)
- `reports/<id>.md` collection reports (collector agent narrative + gaps)
- `reports/<id>-review.md` verification report (review agent checks)

## Open Decisions (You Own)

1. Project name for the public launch (keep “Harness Matrix” as working title?)
2. Hosting later: GitHub Pages vs Vercel
3. Whether to assign an overall “recommendation score” in V1 (I recommend: **no**, only a picker rubric)

